# -*- coding: utf-8 -*-
"""Copie de Antenna array 2D beam forming ANN - SNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18fJu0harcdhmIZ4HtullT91WQjtf15Pe

# Intro

This notebook steps through a simple example of a two-dimensional array of
antennas forming a single narrow beam for a single frequency towards a target angle.

It shows three different methods of optimisation to arrive at feed parameters (amplitudes and phases) for the array:


1.   Standard iterative gradient descent optimiser.
2.   Training an Artifical Neural Network (ANN).
3.   Training a Spiking Neural Network (SNN).

We ignore the fact that the scenario is so simple as to possibly permit analytic solutions, since the aim will be to increase the complexity of the scenario towards a realistic one for satellite communications.

We include basic approaches to estimating energy consumption for ANN vs SNN, where these initial estimates are based on principles rather than in reference to any specific computational hardware.
"""



"""# Antenna array definition

Define the geometry of a 2D antenna array.
"""

import numpy as np
import matplotlib.pyplot as plt

# The antenna coords are pre-computed
antenna_coords = np.array([[0.0, 0.0], [0.03, 0.0], [0.015, 0.025], [-0.014, 0.025], [-0.03, 0.0], [-0.015, -0.025], [0.015, -0.025], [0.06, 0.0], [0.051, 0.029], [0.03, 0.051], [0.0, 0.06], [-0.029, 0.051], [-0.051, 0.029], [-0.06, 0.0], [-0.051, -0.029], [-0.03, -0.051], [0.0, -0.06], [0.03, -0.051], [0.051, -0.03], [0.09, 0.0], [0.084, 0.03], [0.068, 0.057], [0.045, 0.077], [0.015, 0.088], [-0.015, 0.088], [-0.044, 0.077], [-0.068, 0.057], [-0.084, 0.03], [-0.09, 0.0], [-0.084, -0.03], [-0.068, -0.057], [-0.045, -0.077], [-0.015, -0.088], [0.015, -0.088], [0.044, -0.077], [0.068, -0.057], [0.084, -0.03], [0.12, 0.0], [0.115, 0.031], [0.103, 0.059], [0.084, 0.084], [0.06, 0.103], [0.031, 0.115], [0.0, 0.12], [-0.031, 0.115], [-0.059, 0.103], [-0.084, 0.084], [-0.103, 0.059], [-0.115, 0.031], [-0.12, 0.0], [-0.115, -0.031], [-0.103, -0.059], [-0.084, -0.084], [-0.06, -0.103], [-0.031, -0.115], [0.0, -0.12], [0.031, -0.115], [0.06, -0.103], [0.084, -0.084], [0.103, -0.06], [0.115, -0.031], [0.15, 0.0], [0.146, 0.031], [0.137, 0.061], [0.121, 0.088], [0.1, 0.111], [0.075, 0.129], [0.046, 0.142], [0.015, 0.149], [-0.015, 0.149], [-0.046, 0.142], [-0.074, 0.129], [-0.1, 0.111], [-0.121, 0.088], [-0.137, 0.061], [-0.146, 0.031], [-0.15, 0.0], [-0.146, -0.031], [-0.137, -0.061], [-0.121, -0.088], [-0.1, -0.111], [-0.075, -0.129], [-0.046, -0.142], [-0.015, -0.149], [0.015, -0.149], [0.046, -0.142], [0.075, -0.129], [0.1, -0.111], [0.121, -0.088], [0.137, -0.061], [0.146, -0.031], [0.18, 0.0], [0.177, 0.031], [0.169, 0.061], [0.155, 0.089], [0.137, 0.115], [0.115, 0.137], [0.09, 0.155], [0.061, 0.169], [0.031, 0.177], [0.0, 0.18], [-0.031, 0.177], [-0.061, 0.169], [-0.089, 0.155], [-0.115, 0.137], [-0.137, 0.115], [-0.155, 0.09], [-0.169, 0.061], [-0.177, 0.031], [-0.18, 0.0], [-0.177, -0.031], [-0.169, -0.061], [-0.155, -0.09], [-0.137, -0.115], [-0.115, -0.137], [-0.09, -0.155], [-0.061, -0.169], [-0.031, -0.177], [0.0, -0.18], [0.031, -0.177], [0.061, -0.169], [0.089, -0.155], [0.115, -0.137], [0.137, -0.115], [0.155, -0.089], [0.169, -0.061], [0.177, -0.031], [0.21, 0.0], [0.207, 0.031], [0.2, 0.061], [0.189, 0.091], [0.173, 0.118], [0.153, 0.142], [0.13, 0.164], [0.105, 0.181], [0.076, 0.195], [0.046, 0.204], [0.015, 0.209], [-0.015, 0.209], [-0.046, 0.204], [-0.076, 0.195], [-0.104, 0.181], [-0.13, 0.164], [-0.153, 0.142], [-0.173, 0.118], [-0.189, 0.091], [-0.2, 0.061], [-0.207, 0.031], [-0.21, 0.0], [-0.207, -0.031], [-0.2, -0.061], [-0.189, -0.091], [-0.173, -0.118], [-0.153, -0.142], [-0.13, -0.164], [-0.105, -0.181], [-0.076, -0.195], [-0.046, -0.204], [-0.015, -0.209], [0.015, -0.209], [0.046, -0.204], [0.076, -0.195], [0.104, -0.181], [0.13, -0.164], [0.153, -0.142], [0.173, -0.118], [0.189, -0.091], [0.2, -0.061], [0.207, -0.031], [0.24, 0.0], [0.237, 0.031], [0.231, 0.062], [0.221, 0.091], [0.207, 0.119], [0.19, 0.146], [0.169, 0.169], [0.146, 0.19], [0.12, 0.207], [0.091, 0.221], [0.062, 0.231], [0.031, 0.237], [0.0, 0.24], [-0.031, 0.237], [-0.062, 0.231], [-0.091, 0.221], [-0.119, 0.207], [-0.146, 0.19], [-0.169, 0.169], [-0.19, 0.146], [-0.207, 0.119], [-0.221, 0.091], [-0.231, 0.062], [-0.237, 0.031], [-0.24, 0.0], [-0.237, -0.031], [-0.231, -0.062], [-0.221, -0.091], [-0.207, -0.119], [-0.19, -0.146], [-0.169, -0.169], [-0.146, -0.19], [-0.12, -0.207], [-0.091, -0.221], [-0.062, -0.231], [-0.031, -0.237], [0.0, -0.24], [0.031, -0.237], [0.062, -0.231], [0.091, -0.221], [0.12, -0.207], [0.146, -0.19], [0.169, -0.169], [0.19, -0.146], [0.207, -0.12], [0.221, -0.091], [0.231, -0.062], [0.237, -0.031], [0.27, 0.0], [0.268, 0.031], [0.262, 0.062], [0.253, 0.092], [0.241, 0.121], [0.225, 0.148], [0.206, 0.173], [0.185, 0.196], [0.161, 0.216], [0.135, 0.233], [0.106, 0.247], [0.077, 0.258], [0.046, 0.265], [0.015, 0.269], [-0.015, 0.269], [-0.046, 0.265], [-0.077, 0.258], [-0.106, 0.247], [-0.134, 0.233], [-0.161, 0.216], [-0.185, 0.196], [-0.206, 0.173], [-0.225, 0.148], [-0.241, 0.121], [-0.253, 0.092], [-0.262, 0.062], [-0.268, 0.031], [-0.27, 0.0], [-0.268, -0.031], [-0.262, -0.062], [-0.253, -0.092], [-0.241, -0.121], [-0.225, -0.148], [-0.206, -0.173], [-0.185, -0.196], [-0.161, -0.216], [-0.135, -0.233], [-0.106, -0.247], [-0.077, -0.258], [-0.046, -0.265], [-0.015, -0.269], [0.015, -0.269], [0.046, -0.265], [0.077, -0.258], [0.106, -0.247], [0.135, -0.233], [0.161, -0.216], [0.185, -0.196], [0.206, -0.173], [0.225, -0.148], [0.241, -0.121], [0.253, -0.092], [0.262, -0.062], [0.268, -0.031], [0.3, 0.0], [0.298, 0.031], [0.293, 0.062], [0.285, 0.092], [0.274, 0.122], [0.259, 0.149], [0.242, 0.176], [0.222, 0.2], [0.2, 0.222], [0.176, 0.242], [0.15, 0.259], [0.122, 0.274], [0.092, 0.285], [0.062, 0.293], [0.031, 0.298], [0.0, 0.3], [-0.031, 0.298], [-0.062, 0.293], [-0.092, 0.285], [-0.122, 0.274], [-0.149, 0.259], [-0.176, 0.242], [-0.2, 0.222], [-0.222, 0.2], [-0.242, 0.176], [-0.259, 0.149], [-0.274, 0.122], [-0.285, 0.092], [-0.293, 0.062], [-0.298, 0.031], [-0.3, 0.0], [-0.298, -0.031], [-0.293, -0.062], [-0.285, -0.092], [-0.274, -0.122], [-0.259, -0.149], [-0.242, -0.176], [-0.222, -0.2], [-0.2, -0.222], [-0.176, -0.242], [-0.15, -0.259], [-0.122, -0.274], [-0.092, -0.285], [-0.062, -0.293], [-0.031, -0.298], [0.0, -0.3], [0.031, -0.298], [0.062, -0.293], [0.092, -0.285], [0.122, -0.274], [0.15, -0.259], [0.176, -0.242], [0.2, -0.222], [0.222, -0.2], [0.242, -0.176], [0.259, -0.149], [0.274, -0.122], [0.285, -0.092], [0.293, -0.062], [0.298, -0.031], [0.33, 0.0], [0.328, 0.031], [0.324, 0.062], [0.316, 0.092], [0.306, 0.122], [0.293, 0.151], [0.277, 0.178], [0.259, 0.203], [0.238, 0.227], [0.216, 0.249], [0.191, 0.268], [0.165, 0.285], [0.137, 0.3], [0.107, 0.311], [0.077, 0.32], [0.046, 0.326], [0.015, 0.329], [-0.015, 0.329], [-0.046, 0.326], [-0.077, 0.32], [-0.107, 0.311], [-0.137, 0.3], [-0.164, 0.285], [-0.191, 0.268], [-0.216, 0.249], [-0.238, 0.227], [-0.259, 0.203], [-0.277, 0.178], [-0.293, 0.151], [-0.306, 0.122], [-0.316, 0.092], [-0.324, 0.062], [-0.328, 0.031], [-0.33, 0.0], [-0.328, -0.031], [-0.324, -0.062], [-0.316, -0.092], [-0.306, -0.122], [-0.293, -0.151], [-0.277, -0.178], [-0.259, -0.203], [-0.238, -0.227], [-0.216, -0.249], [-0.191, -0.268], [-0.165, -0.285], [-0.137, -0.3], [-0.107, -0.311], [-0.077, -0.32], [-0.046, -0.326], [-0.015, -0.329], [0.015, -0.329], [0.046, -0.326], [0.077, -0.32], [0.107, -0.311], [0.137, -0.3], [0.165, -0.285], [0.191, -0.268], [0.216, -0.249], [0.238, -0.227], [0.259, -0.203], [0.277, -0.178], [0.293, -0.151], [0.306, -0.122], [0.316, -0.092], [0.324, -0.062], [0.328, -0.031], [0.36, 0.0], [0.358, 0.031], [0.354, 0.062], [0.347, 0.093], [0.338, 0.123], [0.326, 0.152], [0.311, 0.179], [0.294, 0.206], [0.275, 0.231], [0.254, 0.254], [0.231, 0.275], [0.206, 0.294], [0.18, 0.311], [0.152, 0.326], [0.123, 0.338], [0.093, 0.347], [0.062, 0.354], [0.031, 0.358], [0.0, 0.36], [-0.031, 0.358], [-0.062, 0.354], [-0.093, 0.347], [-0.123, 0.338], [-0.152, 0.326], [-0.179, 0.311], [-0.206, 0.294], [-0.231, 0.275], [-0.254, 0.254], [-0.275, 0.231], [-0.294, 0.206], [-0.311, 0.18], [-0.326, 0.152], [-0.338, 0.123], [-0.347, 0.093], [-0.354, 0.062], [-0.358, 0.031], [-0.36, 0.0], [-0.358, -0.031], [-0.354, -0.062], [-0.347, -0.093], [-0.338, -0.123], [-0.326, -0.152], [-0.311, -0.18], [-0.294, -0.206], [-0.275, -0.231], [-0.254, -0.254], [-0.231, -0.275], [-0.206, -0.294], [-0.18, -0.311], [-0.152, -0.326], [-0.123, -0.338], [-0.093, -0.347], [-0.062, -0.354], [-0.031, -0.358], [0.0, -0.36], [0.031, -0.358], [0.062, -0.354], [0.093, -0.347], [0.123, -0.338], [0.152, -0.326], [0.179, -0.311], [0.206, -0.294], [0.231, -0.275], [0.254, -0.254], [0.275, -0.231], [0.294, -0.206], [0.311, -0.179], [0.326, -0.152], [0.338, -0.123], [0.347, -0.093], [0.354, -0.062], [0.358, -0.031], [0.39, 0.0], [0.388, 0.031], [0.384, 0.062], [0.378, 0.093], [0.369, 0.123], [0.358, 0.152], [0.345, 0.181], [0.329, 0.208], [0.311, 0.234], [0.291, 0.258], [0.27, 0.281], [0.246, 0.302], [0.221, 0.32], [0.194, 0.337], [0.167, 0.352], [0.138, 0.364], [0.108, 0.374], [0.078, 0.382], [0.047, 0.387], [0.015, 0.389], [-0.015, 0.389], [-0.047, 0.387], [-0.078, 0.382], [-0.108, 0.374], [-0.138, 0.364], [-0.167, 0.352], [-0.195, 0.337], [-0.221, 0.32], [-0.246, 0.302], [-0.27, 0.281], [-0.291, 0.258], [-0.311, 0.234], [-0.329, 0.208], [-0.345, 0.181], [-0.358, 0.152], [-0.369, 0.123], [-0.378, 0.093], [-0.384, 0.062], [-0.388, 0.031], [-0.39, 0.0], [-0.388, -0.031], [-0.384, -0.062], [-0.378, -0.093], [-0.369, -0.123], [-0.358, -0.152], [-0.345, -0.181], [-0.329, -0.208], [-0.311, -0.234], [-0.291, -0.258], [-0.27, -0.281], [-0.246, -0.302], [-0.221, -0.32], [-0.194, -0.337], [-0.167, -0.352], [-0.138, -0.364], [-0.108, -0.374], [-0.078, -0.382], [-0.047, -0.387], [-0.015, -0.389], [0.015, -0.389], [0.047, -0.387], [0.078, -0.382], [0.108, -0.374], [0.138, -0.364], [0.167, -0.352], [0.194, -0.337], [0.221, -0.32], [0.246, -0.302], [0.27, -0.281], [0.291, -0.258], [0.311, -0.234], [0.329, -0.208], [0.345, -0.181], [0.358, -0.152], [0.369, -0.123], [0.378, -0.093], [0.384, -0.062], [0.388, -0.031]])
num_antennas = antenna_coords.shape[0]

# Alternatively, choose random antenna placements
#num_antennas = 50
#max_array_dimension = 0.8 # m
#antenna_coords = (np.random.random((num_antennas, 2)) - 0.5) * max_array_dimension

# Alternatively, remove most of the coords from the computed set
antenna_coords = antenna_coords[0:-1:10]
num_antennas = antenna_coords.shape[0]

print('Num antennas:', num_antennas)

plt.scatter(antenna_coords[:, 0], antenna_coords[:, 1])
plt.xlabel('x (m)')
plt.ylabel('y (m)')
plt.title('Positions of antennas in array')

""" Choose some random feed parameters for the array, and calculate the resulting far-field gain."""

# Define main params
min_gain = -40 # dBi Any gain lower than this we consider inconsequential (and we don't plot it either)
max_angle_of_interest = 90 # degrees; angle either side of centre in one direction
    # to target earth from GEO orbit, this would be about 8.5 deg

frequency = 6e9 # Hz
speed_of_light = 3e8 # m/s
wavelength = speed_of_light / frequency # m

num_sample_angles = 101

def rad(angle_deg):
  return np.radians(angle_deg)

def deg(angle_rad):
  return np.degrees(angle_rad)

# Angles at which we sample the far-field pattern.
# We use UV space sampling,
# which samples angles more densely towards the centre of the range of interest.
max_u = np.sin(rad(max_angle_of_interest))
sample_angles_u = np.linspace(-max_u, max_u, num_sample_angles)
sample_angles_v = sample_angles_u
sample_angles_x = np.arcsin(sample_angles_u) * 180 / np.pi
sample_angles_y = sample_angles_x

grid_coords_u, grid_coords_v = np.meshgrid(sample_angles_u, sample_angles_v)

sample_angles = deg(np.arcsin(sample_angles_u))

c=299792458
freq = 30e9
wavelength = c / freq

k = 2 * np.pi / wavelength
factor_scaling = k / 100

# Elementary feed antenna pattern
antenna_pattern = np.zeros((num_antennas, num_sample_angles, num_sample_angles), dtype="complex64")

for i in range(num_antennas):
  antenna_pattern[i, :, :] = np.exp(1j / wavelength * (
                              antenna_coords[i, 0] * grid_coords_u
                              + antenna_coords[i, 1] * grid_coords_v))

def get_directive_gain_from_feed_params(feed_params):
    gain = np.zeros((num_sample_angles, num_sample_angles))
    for i in range(num_antennas):
        gain = gain + feed_params[i] * antenna_pattern[i, :, :]
    gain = 20 * np.log10(np.abs(gain))
    normalised_gain = gain - np.max(gain)
    return np.clip(normalised_gain, min_gain, None)

# Generate random feed parameters
feed_params_initial = (np.random.random(num_antennas) +
                      (np.random.random(num_antennas) * 2 - 1) * 1j)
# feed_params_initial = np.ones(num_antennas) # alternative feed_params to create a known pattern

#print("Initial feed parameters: ", feed_params_initial)

directive_gain_initial = get_directive_gain_from_feed_params(feed_params_initial)

"""Define plot functions.
Plot the directive gain.

---


"""

from mpl_toolkits.axes_grid1 import make_axes_locatable
from mpl_toolkits import mplot3d

# 2D heatmap visualisation
def plot_far_field_uv(gain, figAx=None):
    if figAx is None:
        plt.close('all')
        fig, ax = plt.subplots()
    else:
        fig, ax = figAx
    divider = make_axes_locatable(ax)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    image = ax.imshow(gain, cmap='viridis')
    ax.set_xlabel('U (sine of azimuth)')
    ax.set_ylabel('V (sine of elevation)')
    ax.set_title('directive gain (dB)')
    fig.colorbar(image, cax=cax, orientation='vertical')

def plot_target_far_field_uv(target_far_field, figAx=None):
    if figAx is None:
        plt.close('all')
        fig, ax = plt.subplots()
    else:
        fig, ax = figAx
    image = ax.imshow(target_far_field, cmap='viridis')
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title('Target far field (Boolean)')

# 3D surface plot
def plot_far_field_uv_3d():
    fig = plt.figure()
    ax = plt.axes(projection='3d')
    ax.plot_surface(grid_coords_u, grid_coords_v, directive_gain_initial,
                    cmap='viridis', edgecolor='none')
    ax.set_xlabel('U (sine of azimuth)')
    ax.set_ylabel('V (sine of elevation)')
    ax.set_zlabel('Normalised directive gain (dB)')

# def plot_target(target_angle, target_beam_width, figAx=None):
#     if figAx is None:
#         plt.close('all')
#         fig, ax = plot_polar()
#     else:
#         fig, ax = figAx
#     # plot target beam boundaries
#     target_beam_boundaries_and_centre = (np.array([rad(target_angle)] * 3)
#                             + np.array([-1, 0, 1]) * rad(target_beam_width) / 2)
#     ax.plot(target_beam_boundaries_and_centre,  [0, min_gain, 0], 'r')
#     return fig, ax

plot_far_field_uv(directive_gain_initial)

"""# Beam target definition
Choose a target direction and width for the beam.
"""

# How wide do we want the beam to be
target_beam_width = 10 # deg

# Where do we want the beam to point? (random)
target_angle_x = (np.random.random(1) - 0.5) * max_angle_of_interest * 2
target_angle_y = (np.random.random(1) - 0.5) * max_angle_of_interest * 2

print('Target angle (x, y): ', target_angle_x, target_angle_y)

def get_target_far_field(target_angle_x, target_angle_y, target_beam_width):
    target_far_field = np.zeros((num_sample_angles, num_sample_angles), dtype=bool)
    dist_x = sample_angles_x - target_angle_x
    dist_y = sample_angles_y - target_angle_y
    dist_x_array = np.tile(dist_x[:, np.newaxis], (1, num_sample_angles))
    dist_y_array = np.tile(dist_y[np.newaxis, :], (num_sample_angles, 1))
    dist = np.sqrt(dist_x_array ** 2 + dist_y_array ** 2)
    target_far_field[dist < target_beam_width / 2] = True
    # catch the case where no sample angle falls within the target beam
    if not np.any(target_far_field):
        # Just find the single closest sample angle
        ids_of_target_angles_x = np.argmin(dist, axis=0)[0]
        ids_of_target_angles_y = np.argmin(dist, axis=1)[1]
        target_far_field[ids_of_target_angles_x, ids_of_target_angles_y] = True
    return target_far_field

target_far_field = get_target_far_field(target_angle_x, target_angle_y, target_beam_width)

fig, axes = plt.subplots(1, 2)
plot_far_field_uv(directive_gain_initial, figAx=(fig, axes[0]))
plot_target_far_field_uv(target_far_field, figAx=(fig, axes[1]))

"""# ANN (using pytorch)
Our basic ANN approach is to have the target far field pattern as input, and the feed params as outputs (with *num_antennas* complex weights expanded out to *2 x num_antennas* real weights). The far-field pattern is simply 1 for sample angles within the desired beam and 0 for angles outside of it.

The far-field pattern is fundamentally 2D, but we reduce it to 1D as an input to the network. We could perhaps do better by retaining the 2D and having a convolutional layer at the input to the network.
"""

import math, scipy.optimize

def real_to_complex(z): # real vector of length 2n -> complex of length n
      return z[:len(z)//2] + 1j * z[len(z)//2:]

def complex_to_real(z):      # complex vector of length n -> real of length 2n
    return np.concatenate((np.real(z), np.imag(z)))

"""Functions to convert between real network outputs and complex feed params redefinition for torch."""

def real_to_complex_torch(z):      # real vector of length 2n -> complex of length n
    return z[:len(z)//2] + 1j * z[len(z)//2:]

def complex_to_real_torch(z):      # complex vector of length n -> real of length 2n
    return torch.concatenate((torch.real(z), torch.imag(z)))

"""## Generating the dataset
We build a dataset of target far fields which are going to be used as inputs to our model and target angles; it is redefined here to work with torch datatypes




"""

import torch

def get_target_far_field(target_angle_x, target_angle_y, target_beam_width):
    target_far_field = np.zeros((num_sample_angles, num_sample_angles), dtype=bool)
    dist_x = sample_angles_x - target_angle_x
    dist_y = sample_angles_y - target_angle_y
    dist_x_array = np.tile(dist_x[:, np.newaxis], (1, num_sample_angles))
    dist_y_array = np.tile(dist_y[np.newaxis, :], (num_sample_angles, 1))
    dist = np.sqrt(dist_x_array ** 2 + dist_y_array ** 2)
    target_far_field[dist < target_beam_width / 2] = True
    # catch the case where no sample angle falls within the target beam
    if not np.any(target_far_field):
        # Just find the single closest sample angle
        ids_of_target_angles_x = np.argmin(dist, axis=0)[0]
        ids_of_target_angles_y = np.argmin(dist, axis=1)[1]
        target_far_field[ids_of_target_angles_x, ids_of_target_angles_y] = True
    return target_far_field

# TODO make this a function that works on batches
def get_target_far_field_torch(target_angle_x, target_angle_y, target_beam_width=1.0):
    target_far_field = torch.zeros((num_sample_angles, num_sample_angles))
    dist_x = torch.from_numpy(sample_angles_x - target_angle_x)
    dist_y = torch.from_numpy(sample_angles_y - target_angle_y)
    dist_x_array = torch.tile(dist_x[:, None], (1, num_sample_angles))
    dist_y_array = torch.tile(dist_y[None, :], (num_sample_angles, 1))
    dist = torch.sqrt(dist_x_array ** 2 + dist_y_array ** 2)
    target_far_field[dist < target_beam_width / 2] = 1.
    # catch the case where no sample angle falls within the target beam
    if not torch.any(target_far_field):
        # Just find the single closest sample angle
        # Just find the single closest sample angle
        ids_of_target_angles_x = np.argmin(dist, axis=0)[0]
        ids_of_target_angles_y = np.argmin(dist, axis=1)[1]
        target_far_field[ids_of_target_angles_x, ids_of_target_angles_y] = 1.

    return torch.flatten(target_far_field) # we want a 1D array input for simplicity
    # TODO: like this, we don't get to benefit from 2D convolutions - consider reformulating

# generate dataset
dataset_size = 1000
max_angle_of_interest = 90

target_angles = (np.random.random((dataset_size, 2)) - 0.5) * (90 - target_beam_width / 2) * 2

inputs = []
for angle_pair in target_angles:
    inputs.append(get_target_far_field_torch(target_angle_x=angle_pair[0],
                                             target_angle_y=angle_pair[1],
                                             target_beam_width=target_beam_width))
input_tensor = torch.stack(inputs)
split_index = int(0.9*dataset_size)
train_input = input_tensor[:split_index]
train_angles = target_angles[:split_index]
test_input = input_tensor[split_index:]
test_angles = target_angles[split_index:]

"""Redefine the gain calculation to work with torch data types."""

antenna_pattern_torch = torch.transpose(torch.flatten(torch.from_numpy(antenna_pattern), start_dim=1, end_dim=2), 0, 1)

def get_directive_gain_from_feed_params_torch(feed_params):
    gain = 20 * torch.log10(torch.abs(torch.mv(antenna_pattern_torch, feed_params)))
    normalised_gain = gain - torch.max(gain)
    clamped_normalised_gain = torch.clamp(normalised_gain, min=min_gain)
    return clamped_normalised_gain

"""Define a cost function for the far-field pattern which results from feed params which output from the network. The cost function refers to the target input and gives a value between 0 (good) and 1 (bad)."""

def get_cost_torch(directive_gain, target_input):
    good_gain = 1 - directive_gain[target_input.bool()] / min_gain
    bad_gain = 1 - torch.clamp(directive_gain[~target_input.bool()], min=min_gain) / min_gain
    cost = (1 - torch.mean(good_gain) + torch.mean(bad_gain)) / 2
    return cost

"""Define an Artificial Neural Network (ANN).
The chosen baseline architecture is a fully connected feed-forward neural network with one hidden layer, with ReLU activation.
Use the Adam optimiser, and batch size of 10.
Run the optimiser for 100 epochs.
"""

import torch.nn as nn

hidden_dim = 256
batch_size = dataset_size
use_bias = True
model = nn.Sequential(
    nn.Linear(in_features=train_input.shape[1], out_features=hidden_dim, bias=use_bias),
    nn.ReLU(),
    nn.Linear(in_features=hidden_dim, out_features=num_antennas*2, bias=use_bias)
)

# Optionally, the simplest solution, with no hidden layers:
#model = nn.Linear(in_features=train_input.shape[1], out_features=num_antennas*2)

# Optionally, include more hidden layers:
# model = nn.Sequential(
#    nn.Linear(in_features=len(sample_angles), out_features=hidden_dim, bias=use_bias),
#    nn.ReLU(),
#    nn.Linear(in_features=hidden_dim, out_features=hidden_dim),
#    nn.ReLU(),
#    nn.Linear(in_features=hidden_dim, out_features=num_antennas*2, bias=use_bias)
#)

optim = torch.optim.Adam(model.parameters(), lr=1e-3)

plot_every = 1
for epoch in range(20):
    model.zero_grad()

    # calculate output
    params_for_batch = model(train_input)

    # generate loss for every element of batch
    train_loss = 0
    for params, input_far_field in zip(params_for_batch, train_input):
        #print('Params: ', params)
        complex_params = real_to_complex_torch(params)
        directive_gain = get_directive_gain_from_feed_params_torch(complex_params)
        single_output_cost = get_cost_torch(directive_gain, input_far_field)
        train_loss = train_loss + single_output_cost
    train_loss.backward()
    optim.step()
    if epoch % plot_every == 0:
      with torch.no_grad():
        test_params = model(test_input)
        # generate loss for every element of batch
        test_loss = 0
        for params, input_far_field in zip(test_params, test_input):
            complex_params = real_to_complex_torch(params)
            directive_gain = get_directive_gain_from_feed_params_torch(complex_params)
            single_output_cost = get_cost_torch(directive_gain, input_far_field)
            test_loss = test_loss + single_output_cost
      print(f"Epoch {epoch}: train loss {train_loss.item()/train_input.shape[0]}, test loss {test_loss.item()/test_input.shape[0]}")

"""## ANN results visualisation"""

def plot_batched_far_fields(feed_params, target_angles):
    n_plots = feed_params.shape[0]
    fig, axes = plt.subplots(n_plots, 2)
    for idx in range(n_plots):
        coeffs = feed_params[idx]
        target_far_field = get_target_far_field(target_angles[idx, 0], target_angles[idx, 1], target_beam_width)
        directive_gain = get_directive_gain_from_feed_params(real_to_complex(coeffs.clone().detach().numpy()))
        plot_target_far_field_uv(target_far_field, figAx=(fig, axes[idx, 0]))
        plot_far_field_uv(directive_gain, figAx=(fig, axes[idx, 1]))

def plot_far_field_example(coeffs, target_angles):
    fig, axes = plt.subplots(1, 2)
    target_far_field = get_target_far_field(target_angles[0], target_angles[1], target_beam_width)
    directive_gain = get_directive_gain_from_feed_params(real_to_complex(coeffs.clone().detach().numpy()))
    plot_target_far_field_uv(target_far_field, figAx=(fig, axes[0]))
    plot_far_field_uv(directive_gain, figAx=(fig, axes[1]))


# Performance of TRAINING set

subset = 2
#plot_batched_far_fields(params_for_batch[:subset], train_angles[:subset])
plot_far_field_example(params_for_batch[1], train_angles[1])

# Performance on test set

#plot_batched_far_fields(test_params[:subset], test_angles[:subset])
plot_far_field_example(params_for_batch[1], train_angles[1])

"""## ANN computation estimation"""

# install a package for FLOP counters for ANNs
!pip install fvcore --quiet

"""Count the floating-point operations in the ANN."""

from fvcore.nn import FlopCountAnalysis

flops = FlopCountAnalysis(model, train_input)
total_flops = flops.total() // train_input.shape[0]
print(f"The ANN uses {total_flops} floating-point operations.")

"""# SNN (using sinabs)"""

!pip install sinabs --quiet

"""Convert the ANN to an SNN."""

train_input.shape

import sinabs

snn = sinabs.from_torch.from_model(model, batch_size=test_input.shape[0], spike_threshold=0.01).spiking_model
print(snn)

spikes = []
def record_spikes(module, input_, output):
    spikes.append(output)

n_steps = 5
timed_input = test_input.unsqueeze(1).repeat((1, n_steps, 1))
print(timed_input.shape)
snn[1].register_forward_hook(record_spikes)
with torch.no_grad():
    snn_out = snn(timed_input).sum(1) # here we sum the output over the time dimension in order to make it similar to the ANN output

with torch.no_grad():
  ann_output = model(test_input)

snn_out_scale_factor = (snn_out / ann_output).mean()
print(f"Scale factor for SNN output is {snn_out_scale_factor}")

"""## SNN visualisation"""

subset = 3
plot_batched_far_fields(snn_out[:subset] / snn_out_scale_factor, test_angles[:subset])

plot_far_field_example(snn_out[1], test_angles[1])

"""# Energy comparison ANN vs SNN

Now let's estimate the energy that both ANN and SNN would use!

For two operands of bitwidth b1 and b2, the cost of addition is max(b1, b2) + (b1âˆ’b2) / 2 and a MAC consumes b1*b2!
"""

energy_add = lambda b1, b2: max(b1, b2) + abs(b1-b2)/2
energy_mac = lambda b1, b2: b1*b2

intermediate_spikes = spikes[0].sum((1,2)).mean()
num_spikes = train_input[0].sum() * hidden_dim + intermediate_spikes * 24

energy_snn = num_spikes * energy_add(32, 32)
energy_ann = total_flops * energy_mac(16, 16)

plt.bar(["ANN", "SNN"], [1, energy_snn/energy_ann])
plt.yscale('log')
plt.title("Relative dynamic energy")

print("Energy ratio ANN/SNN: ", float(energy_ann/energy_snn))